"""
This module provides an implementation of the Iteration of Thought (IoT) framework for generating responses using OpenAI's API.
"""

import os
import time
from openai import OpenAI, OpenAIError  # Import OpenAIError
from loguru import logger  # Add this import

API_KEY = os.getenv("OPENAI_API_KEY")
MODEL = "gpt-4o-mini"
RATE_LIMIT_WAIT_TIME = 10  # Time to wait on rate limit error

# Ensure the OpenAI API key is set
if not API_KEY:
    raise ValueError("OpenAI API key must be set as an environment variable.")

client = OpenAI(api_key=API_KEY)


class IterationOfThought:
    """
    Class to manage the iteration of thought processes using a specified model.
    """

    def __init__(self, model=MODEL, max_iterations=5, timeout=30):
        """
        Initialize the IoT framework.

        :param model: The OpenAI model to use (e.g., 'gpt-4o-mini').
        :param max_iterations: Maximum number of iterations for GIoT.
        :param timeout: Time in seconds to wait between API calls to avoid rate limits.
        """
        self.model = model
        self.max_iterations = max_iterations
        self.timeout = timeout

    def _call_openai(self, prompt, temperature=0.5):
        """
        Call the OpenAI API with the given prompt.

        :param prompt: The prompt to send to the model.
        :param temperature: Sampling temperature.
        :return: The model's response text or an empty string on failure.
        """
        logger.debug(f"Calling OpenAI API with prompt: {prompt}")  # Add debug log

        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
            )
            return response.choices[0].message.content.strip()  # Update access method
        except OpenAIError as e:  # Use OpenAIError instead of openai.OpenAIError
            logger.error(f"An error occurred while calling OpenAI API: {e}")
            print(f"Rate limit exceeded. Waiting for {RATE_LIMIT_WAIT_TIME} seconds.")
            time.sleep(RATE_LIMIT_WAIT_TIME)
            return self._call_openai(prompt, temperature)
        except (ValueError, TypeError) as e:  # Catch specific exceptions
            logger.error(f"An error occurred while calling OpenAI API: {e}")
            print(f"An error occurred while calling OpenAI API: {e}")
            return ""
        except Exception as e:  # Catch all other exceptions
            logger.error(f"An unexpected error occurred: {e}")
            print(f"An unexpected error occurred: {e}")
            return ""

    def inner_dialogue_agent(self, query, previous_response):
        """
        Generate a new prompt based on the query and previous response.

        :param query: The original user query.
        :param previous_response: The LLM's previous response.
        :return: A new prompt for refining the response.
        """
        prompt = (
            f"Given the original query: '{query}' and the previous response: '{previous_response}', "
            "generate an instructive and context-specific prompt to refine and improve the answer. "
            "Ensure that the new prompt encourages deeper reasoning or addresses any gaps in the previous response."
        )

        logger.debug(f"Generated prompt for IDA: {prompt}")  # Add debug log
        return self._call_openai(prompt)

    def llm_agent(self, query, prompt):
        """
        Generate a refined response based on the query and the prompt.

        :param query: The original user query.
        :param prompt: The prompt generated by the IDA.
        :return: The refined response from the LLM.
        """
        full_prompt = f"Query: {query}\nPrompt: {prompt}\nResponse:"

        logger.debug(f"Full prompt for LLM: {full_prompt}")  # Add debug log
        return self._call_openai(full_prompt)

    def stopping_criterion(self, response):
        """
        Determine whether to stop iterating based on the response.

        :param response: The current response from the LLM.
        :return: True if stopping condition is met, False otherwise.
        """
        lower_response = response.lower()

        return "answer:" in lower_response or "final answer:" in lower_response

    def aiot(self, query):
        """
        Implement the Autonomous Iteration of Thought (AIoT) variant.

        :param query: The original user query.
        :return: The final refined response.
        """
        logger.info("Starting AIoT...")  # Change print to logger
        current_response = self.llm_agent(query, "Initial Prompt")

        for iteration in range(1, self.max_iterations + 1):
            logger.info(f"\nIteration {iteration}:")  # Change print to logger
            logger.info(
                f"LLMA Response:\n{current_response}\n"
            )  # Change print to logger

            if self.stopping_criterion(current_response):
                logger.info("Stopping criterion met.")  # Change print to logger
                break

            new_prompt = self.inner_dialogue_agent(query, current_response)
            logger.info(
                f"IDA Generated Prompt:\n{new_prompt}\n"
            )  # Change print to logger

            current_response = self.llm_agent(query, new_prompt)
            time.sleep(self.timeout)  # To avoid hitting rate limits

        logger.info("AIoT completed.\n")  # Change print to logger
        return current_response

    def giot(self, query, fixed_iterations):
        """
        Implement the Guided Iteration of Thought (GIoT) variant.

        :param query: The original user query.
        :param fixed_iterations: Number of iterations to perform.
        :return: The final refined response.
        """

        logger.info("Starting GIoT...")  # Change print to logger
        current_response = self.llm_agent(query, "Initial Prompt")

        for iteration in range(1, fixed_iterations + 1):
            logger.info(f"\nIteration {iteration}:")  # Change print to logger
            logger.info(
                f"LLMA Response:\n{current_response}\n"
            )  # Change print to logger

            new_prompt = self.inner_dialogue_agent(query, current_response)
            logger.info(
                f"IDA Generated Prompt:\n{new_prompt}\n"
            )  # Change print to logger

            current_response = self.llm_agent(query, new_prompt)
            time.sleep(self.timeout)  # To avoid hitting rate limits

        logger.info("GIoT completed.\n")  # Change print to logger
        return current_response


def main() -> None:
    """
    Main entry point for the IoT agent application.
    """

    logger.add("debug.log", level="DEBUG")  # Add logger configuration

    # Initialize the IoT framework
    iot = IterationOfThought(model=MODEL, max_iterations=5, timeout=2)

    # Define a sample query
    sample_query = (
        "A textile dye containing an extensively conjugated pi-electrons emits light with energy of 2.3393 eV. "
        "What color of light is absorbed by the organic compound? Pick an answer from the following options:\n"
        "A. Red\nB. Yellow\nC. Blue\nD. Violet"
    )

    # Run AIoT
    final_response_aiot = iot.aiot(sample_query)
    logger.info(
        "Final AIoT Response:\n" + final_response_aiot
    )  # Change print to logger

    # Run GIoT with a fixed number of iterations
    final_response_giot = iot.giot(sample_query, fixed_iterations=3)
    logger.info(
        "Final GIoT Response:\n" + final_response_giot
    )  # Change print to logger


# Example Usage
if __name__ == "__main__":
    main()
